{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import time, enum, math\n",
    "import pylab as plt\n",
    "import random\n",
    "\n",
    "from state import State\n",
    "from Agent import MyAgent\n",
    "from NetworkInformationDiffusionModel import NetworkInformationDiffusionModel\n",
    "from Visualization import Visualization\n",
    "from Data import Data\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Post = {'author':2 , 'content': 'A positive Trump post', 'type': 0, 'party_mention': None, 'pov': 0, 'emotion_score':{}}\n",
    "cmap = [\"orange\", \"red\", \"blue\", 'green', 'grey']\n",
    "states = ['origin', 'received', 'not-received', 'spreader', 'distinterested']\n",
    "colors = dict(zip(cmap, states))\n",
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_share_prob(data, potential_users):\n",
    "    \n",
    "    user_id = None\n",
    "    \n",
    "    sharing_prob = potential_users['user_activity'] * potential_users['privacy_preference']\n",
    "    \n",
    "    # 50% of the times make a random selection of author\n",
    "    if(random.random() > 0.50):\n",
    "        user_id = int(data.sample(1)['id'].values[0])\n",
    "        \n",
    "    else:\n",
    "        user_id = int(sharing_prob.idxmax())\n",
    "        \n",
    "    return user_id\n",
    "    \n",
    "def pick_an_author(data, post):\n",
    "    \n",
    "    issue_id = post[0]\n",
    "    post_stance = post[1]\n",
    "    issue_mentioned = 'issue_' + str(int(issue_id))\n",
    "    user_id = None\n",
    "    \n",
    "    if data[data[issue_mentioned] <= 0].shape[0] == 0 or data[data[issue_mentioned] >= 0].shape[0] == 0:\n",
    "        return None\n",
    "    \n",
    "    agent_inclination = data[issue_mentioned]\n",
    "    if(post_stance >= 0):\n",
    "        potential_users = data[data[issue_mentioned] >= 0]\n",
    "        selected_author = compute_share_prob(data, potential_users)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        potential_users = data[data[issue_mentioned] < 0]\n",
    "        selected_author = compute_share_prob(data, potential_users)\n",
    "    \n",
    "    \n",
    "    return selected_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_data(df, data, attr):\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        if(df.iloc[i][attr] != data.iloc[i][attr]):\n",
    "            print(\"old value \", df.iloc[i][attr], \" \\t New value\", data.iloc[i][attr])\n",
    "\n",
    "def update_user_activity(data, G, user, agg_sanct):\n",
    "    \n",
    "    user_data = data.iloc[user]\n",
    "    \n",
    "    old_user_activity = user_data['user_activity']\n",
    "    \n",
    "    ## NEED TO CHANGE THIS FORMULA TO BE SOMETHING MORE MEANINGFUL\n",
    "    new_user_activity = old_user_activity + (agg_sanct * 0.01)\n",
    "    \n",
    "    #Update the user activity with the new value (a bounded value between 0 and 1)\n",
    "    new_user_activity = max(min(1, new_user_activity), 0)\n",
    "    #print(\"Updating \", user, \"\\t\", old_user_activity, \"\\t\", new_user_activity, \"\\t\", agg_sanct)\n",
    "    #data.at[user, 'user_activity'] = new_user_activity\n",
    "    \n",
    "    #print(\"Here \", data['user_activity'].sum(), \"\\t\", G.nodes[user]['user_activity'])\n",
    "    \n",
    "    data['user_activity'].iloc[user] = new_user_activity\n",
    "    G.nodes[user]['user_activity'] = new_user_activity\n",
    "    \n",
    "#     print(\"old_user_activity \", old_user_activity)\n",
    "#     print(\"user_activity\", new_user_activity, \" agg_sanct \", agg_sanct)\n",
    "    \n",
    "#     print(\"Here \", data['user_activity'].sum(), \"\\t\", G.nodes[user]['user_activity'])\n",
    "    \n",
    "    \n",
    "    return data, G\n",
    "    \n",
    "def attitude_shift(sanction_score, att_diff, author_inclination, reciever_inclination):\n",
    "    \n",
    "    attitude_shift = sanction_score * att_diff\n",
    "    if(author_inclination >= reciever_inclination):\n",
    "        new_user_incl = author_inclination - attitude_shift\n",
    "    else:\n",
    "        new_user_incl = author_inclination + attitude_shift\n",
    "        \n",
    "    return new_user_incl\n",
    "\n",
    "\n",
    "def update_user_satisfaction(data, G, user, agg_sanct):\n",
    "    \n",
    "    user_data = data.iloc[user]\n",
    "    \n",
    "    old_user_satisfaction = user_data['user_satisfaction']\n",
    "    \n",
    "    ## NEED TO CHANGE THIS FORMULA TO BE SOMETHING MORE MEANINGFUL\n",
    "    new_user_satisfaction = old_user_satisfaction + (agg_sanct * 0.01)\n",
    "    \n",
    "    data['user_satisfaction'].iloc[user] = new_user_satisfaction\n",
    "    G.nodes[user]['user_satisfaction'] = new_user_satisfaction\n",
    "    \n",
    "#     print(\"old_user_satisfaction \", old_user_satisfaction)\n",
    "#     print(\"user_satisfaction\", new_user_satisfaction, \" agg_sanct \", agg_sanct)\n",
    "    \n",
    "#     print(\"Here \", data['user_satisfaction'].sum(), \"\\t\", G.nodes[user]['user_satisfaction'])\n",
    "#     #print(\"Here \", data['user_satisfaction'].sum())\n",
    "    \n",
    "    #print(\"Here: \", new_user_satisfaction)\n",
    "#     sys.exit()\n",
    "    \n",
    "    return data, G\n",
    "    \n",
    "def update_user_preferences(sG, G, data, issue_id, lower_attd_th, upper_attd_th):\n",
    "    \n",
    "    labels = nx.get_edge_attributes(sG,'weight')\n",
    "    outgoing = list(set([x[0] for x in labels.keys()]))\n",
    "    incoming = list(set([x[1] for x in labels.keys()]))\n",
    "    \n",
    "    for n in incoming:\n",
    "        agg_sanct = sum([labels[x] for x in labels if x[1] == n])\n",
    "        data, G = update_user_activity(data, G, n, agg_sanct)\n",
    "        data, G = update_user_satisfaction(data, G, n, agg_sanct)\n",
    "        \n",
    "    #print(\"agg_sanct : \", agg_sanct)\n",
    "#     print(\"Here \", data['user_activity'].sum())\n",
    "#     sys.exit()\n",
    "    for node in incoming:\n",
    "        \n",
    "        edges = sG.in_edges(node, data=True)\n",
    "        gama = 0\n",
    "        \n",
    "        for x in edges:\n",
    "            reciever = x[0]\n",
    "            author = x[1]\n",
    "            sanction_score = x[2]['weight']\n",
    "\n",
    "            reciever_inclination = data[data['id'] == reciever]['issue_' + str(issue_id)].values[0]\n",
    "            author_inclination = data[data['id'] == author]['issue_' + str(issue_id)].values[0]\n",
    "            att_diff =  abs(reciever_inclination - author_inclination)\n",
    "            \n",
    "#             if sanction_score > 0:\n",
    "#                 sanction_score = 2 * sanction_score\n",
    "                \n",
    "            flag = 0\n",
    "        \n",
    "            if(sjt_flag == False):\n",
    "                 \n",
    "                flag = 1\n",
    "                new_user_incl = attitude_shift(sanction_score, att_diff, author_inclination, reciever_inclination)\n",
    "                   \n",
    "            else:\n",
    "                # If attitude difference is within the latitude of acceptance change attitude towards the provided sanction \n",
    "                if(att_diff <= lower_attd_th):\n",
    "\n",
    "                    flag = 1\n",
    "                    new_user_incl = attitude_shift(sanction_score, att_diff, author_inclination, reciever_inclination)\n",
    "\n",
    "                elif(att_diff >= upper_attd_th):\n",
    "\n",
    "                    flag = 1\n",
    "                    new_user_incl = attitude_shift((-1) * sanction_score, att_diff, author_inclination, reciever_inclination)\n",
    "\n",
    "            if(flag == 1):\n",
    "\n",
    "                if(new_user_incl <= -1):\n",
    "                    new_user_incl = -1\n",
    "                elif(new_user_incl >= 1):\n",
    "                    new_user_incl = 1\n",
    "\n",
    "                data['issue_' + str(issue_id)].iloc[author] = new_user_incl\n",
    "                G.nodes[author]['issue_' + str(issue_id)] = new_user_incl\n",
    "                \n",
    "    return data, G\n",
    "\n",
    "def update_pol_pol_in_graph(data, G):\n",
    "    \n",
    "    for n in G:\n",
    "        G.nodes[n]['pol_inclination'] = data.iloc[n]['pol_inclination']\n",
    "        \n",
    "    return G\n",
    "\n",
    "\n",
    "def run_simulation(post, G, steps):\n",
    "\n",
    "    model = NetworkInformationDiffusionModel(post, G, se_flag, se_threshold)\n",
    "    for i in range(steps):\n",
    "\n",
    "        model.step(i)\n",
    "        #agent_state = model.datacollector.get_agent_vars_dataframe()\n",
    "        #X = pd.pivot_table(agent_state.reset_index(), index='Step', columns='State', aggfunc=np.size, fill_value=0)  \n",
    "        \n",
    "    #print(model.datacollector.get_agent_vars_dataframe())\n",
    "    agent_state = model.datacollector.get_agent_vars_dataframe()\n",
    "    \n",
    "    states = [int(i.state) for i in model.grid.get_all_cell_contents()]\n",
    "    agents = model.agents\n",
    "    \n",
    "    return model, states, agents, model.G, model.G_share, agent_state\n",
    "\n",
    "\n",
    "def rmse(df):\n",
    "    \n",
    "    x1 = df[df['pol_inclination'] > 0]['pol_inclination'].mean()\n",
    "    x2 = df[df['pol_inclination'] < 0]['pol_inclination'].mean() \n",
    "    rmse = math.sqrt(((x1 * x1) + (x2 * x2))/df.shape[0])\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "\n",
    "def start_simulation(data, G, post_conf, n_issues, lower_attd_th, upper_attd_th):\n",
    "    \n",
    "    steps=50\n",
    "    SimModel, states, agents, AgentGraphs, SancGraphs, AgentStates, runtime = {}, [], [], [], [], [], []\n",
    "    network_homophily = []\n",
    "    user_pol_disposition = []\n",
    "    user_satisfaction = []\n",
    "    polarity = []\n",
    "    i = 0\n",
    "    sharing_details = []\n",
    "\n",
    "    for j, post in post_conf.iterrows():\n",
    "\n",
    "        print(i, end=\"\\r\")\n",
    "        i+=1\n",
    "        \n",
    "        pc = (post['issue'], post['stance'])\n",
    "\n",
    "        st=time.time()\n",
    "        author_id = pick_an_author(data, pc)\n",
    "        #author_id = int(post['author_id'])\n",
    "        \n",
    "        if(author_id == None):\n",
    "            continue\n",
    "\n",
    "        post = dict()\n",
    "        post['author'] = author_id\n",
    "        post['issue'] = int(pc[0])\n",
    "        post['stance'] = pc[1]\n",
    "\n",
    "        SimModel[j], state, agent, G_agents, G_sanctions, agent_state = run_simulation(post, G, steps)\n",
    "        \n",
    "        et = time.time()\n",
    "        rt = round(et-st, 6)\n",
    "\n",
    "        states.append(state) \n",
    "        agents.append(agent)\n",
    "        AgentGraphs.append(G_agents)\n",
    "        SancGraphs.append(G_sanctions)\n",
    "        AgentStates.append(agent_state)\n",
    "        runtime.append(rt)\n",
    "\n",
    "        author = author_id\n",
    "        #received_agents_count = sum([1 for x in state if x == 1])\n",
    "        not_received_agents_count = sum([1 for x in state if x == 2])\n",
    "        spreader_agents_count = sum([1 for x in state if x == 3])\n",
    "        disinterested_agents_counts = sum([1 for x in state if x == 4])\n",
    "        received_agents_count = spreader_agents_count + disinterested_agents_counts\n",
    "\n",
    "        #print(state)\n",
    "        sharing_details.append([pc[0], round(pc[1], 6), author_id, received_agents_count, not_received_agents_count, spreader_agents_count, disinterested_agents_counts])\n",
    "        #print(sharing_details)\n",
    "\n",
    "#         Visualization().plot_sim_network(G_agents, state)\n",
    "#         Visualization().plot_sanction_graph(G_sanctions)\n",
    "\n",
    "        # Update user preferences based on. sanctions received from other agents\n",
    "        data, G_agents = update_user_preferences(G_sanctions, G_agents, data, post['issue'], lower_attd_th, upper_attd_th)\n",
    "        \n",
    "        \n",
    "        pol_inclination = Data.get_agent_pol_inclinations(data, n_issues)\n",
    "        data['pol_inclination'] = pol_inclination\n",
    "        \n",
    "        G_agents = update_pol_pol_in_graph(data, G_agents)\n",
    "        \n",
    "        #print(\"Here \", G_agents.nodes[0])\n",
    "        \n",
    "        #sys.exit()\n",
    "        \n",
    "        net_user_satisfaction = data['user_satisfaction'].mean()\n",
    "        #print(net_user_satisfaction)\n",
    "#         print(net_user_satisfaction)\n",
    "#         sys.exit()\n",
    "        user_satisfaction.append(net_user_satisfaction)\n",
    "        \n",
    "        rep = sum([x*x for x in data[data['pol_inclination'] < 0]['pol_inclination']])\n",
    "        dem = sum([x*x for x in data[data['pol_inclination'] > 0]['pol_inclination']])\n",
    "        pol = round((rep + dem)/data.shape[0], 6) \n",
    "        \n",
    "        pol2 = data[data['pol_inclination'] > 0]['pol_inclination'].mean() + data[data['pol_inclination'] < 0]['pol_inclination'].mean()\n",
    "        pol2 = rmse(data)\n",
    "        temp_G = G.copy()\n",
    "        \n",
    "        node_attr = data.set_index('id').to_dict('index')\n",
    "        nx.set_node_attributes(temp_G, node_attr)\n",
    "        \n",
    "        for n in temp_G.nodes:\n",
    "            if(node_attr[n]['pol_inclination'] >= 0.65 or node_attr[n]['pol_inclination'] <= -0.65):\n",
    "                node_attr[n]['pol_inclination_grp'] = -1\n",
    "            elif((node_attr[n]['pol_inclination'] > -0.65 or node_attr[n]['pol_inclination'] <= -0.25) and (node_attr[n]['pol_inclination'] >= 0.25 or node_attr[n]['pol_inclination'] < 0.65)):\n",
    "                node_attr[n]['pol_inclination_grp'] = 0\n",
    "            elif((node_attr[n]['pol_inclination'] > -0.25 or node_attr[n]['pol_inclination'] < 0.25) and (node_attr[n]['pol_inclination'] > 0.25 or node_attr[n]['pol_inclination'] < 0.65)):\n",
    "                node_attr[n]['pol_inclination_grp'] = 0\n",
    "            \n",
    "        nx.set_node_attributes(temp_G, node_attr)\n",
    "        \n",
    "        hom = nx.attribute_assortativity_coefficient(temp_G, \"pol_inclination_grp\")\n",
    "        user_pol_disposition.append(pol)\n",
    "        network_homophily.append(hom)\n",
    "        polarity.append(pol2)\n",
    "#         print(hom)\n",
    "#         if(i == 10):\n",
    "#             sys.exit()\n",
    "\n",
    "    return data, G_agents, sharing_details, user_pol_disposition, network_homophily, polarity, user_satisfaction\n",
    "\n",
    "    #final_sanc_graph = Visualization().aggregate_sanction_graphs(SancGraphs)\n",
    "    #G_sharing = Visualization().get_sharing_graphs(G, final_sanc_graph)  \n",
    "    \n",
    "def save_results_to_dir(run, epoch, data, sharing_details, net_polarization, network_homophily, polarity, user_satisfaction):\n",
    "    \n",
    "    results_df = pd.DataFrame(sharing_details, columns = ['party_mentioned', 'post_stance', 'author_id', 'num_of_agents_received', 'num_of_agents_not_received', \n",
    "                                                                  'num_of_spreader_agents', 'num_of_disinterested_agents'])\n",
    "    \n",
    "    results_df['network_polarization'] = net_polarization\n",
    "    results_df['network_homophily'] = network_homophily\n",
    "    results_df['network_polarity'] = polarity\n",
    "    results_df['user_satisfaction'] = user_satisfaction\n",
    "    \n",
    "    mypath = '../results/results_' + str(run) + '/'\n",
    "    results_df.to_csv(mypath + 'sharing_details_' + str(epoch) + '.csv')\n",
    "    #data.to_csv(mypath + 'network_data_' + str(epoch) + '.csv')\n",
    "    \n",
    "    return\n",
    "\n",
    "def save_graph(run, i, polarization, flag):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(range(len(polarization)), polarization)\n",
    "    if(flag == 3):\n",
    "        filepath = '../results/results_' + str(run) + '/user_satisfaction_' + str(i) +'.jpg'\n",
    "    if(flag == 2):\n",
    "        filepath = '../results/results_' + str(run) + '/network_polarity_' + str(i) +'.jpg'\n",
    "    elif(flag == 1):\n",
    "        filepath = '../results/results_' + str(run) + '/polarization_' + str(i) +'.jpg'\n",
    "    elif(flag == 0):\n",
    "        filepath = '../results/results_' + str(run) + '/network_homophily_' + str(i) +'.jpg'\n",
    "        \n",
    "    plt.savefig(filepath)\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "    \n",
    "def save_data(run, i, data, flag):\n",
    "    \n",
    "    mypath = '../results/results_' + str(run) + '/'\n",
    "        \n",
    "    if os.path.isdir(mypath) == False:\n",
    "        os.mkdir(mypath)\n",
    "        \n",
    "    if(flag == 1):\n",
    "        data.to_csv(mypath + 'initial_data.csv')\n",
    "    elif(flag == 2):\n",
    "        data.to_csv(mypath + 'final_data_' + str(i) +'.csv')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se_flag = False\n",
    "# sjt_flag= True\n",
    "# se_threshold = 0\n",
    "# lower_attd_th = 0.6\n",
    "# upper_attd_th = 1.4\n",
    "\n",
    "# n_issues= 6\n",
    "# initial_graph, initial_data = Data.get_social_network(n_issues)\n",
    "# post_conf = Data.generate_posts(300, n_issues)\n",
    "\n",
    "# saved_df = pd.read_csv('../results/new_results_tolerance/results_0/initial_data.csv')\n",
    "# initial_graph = Data.get_saved_network(saved_df)\n",
    "# initial_data = saved_df.copy()\n",
    "# sharing_details = pd.read_csv('../results/new_results_tolerance/results_0/results_tollorant_Most.csv')\n",
    "\n",
    "# rep = abs(data[data['pol_inclination'] < 0]['pol_inclination'].sum())\n",
    "# dem = abs(data[data['pol_inclination'] > 0]['pol_inclination'].sum())\n",
    "# initial_pol = round((rep + dem)/data.shape[0], 6) \n",
    "# initial_homophily = nx.attribute_assortativity_coefficient(G, \"pol_inclination\")\n",
    "# initial_pol, initial_homophily\n",
    "\n",
    "# se_threshold = 0.5\n",
    "# lower_attd_th = 0.5\n",
    "# upper_attd_th = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post_conf = sharing_details[['party_mentioned', 'post_stance', 'author_id']].rename(columns = {'party_mentioned': 'issue'})\n",
    "# post_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_data.to_csv('../results/initial_data.csv')\n",
    "# post_conf.to_csv('../results/post_conf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp 1: Varying Selective Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number_of_issues = [2,4,8,16]\n",
    "num_issues = 6\n",
    "sjt_flag= True\n",
    "lower_attd_th = 0.6\n",
    "upper_attd_th = 1.4\n",
    "se_flags = [False, True, True, True]\n",
    "se_thresholds = [0, 0.4, 1.0, 1.6]\n",
    "n_issues = 6\n",
    "initial_graph, initial_data = Data.get_social_network(n_issues)\n",
    "post_conf = Data.generate_posts(4000, num_issues)\n",
    "\n",
    "k=0\n",
    "for i in range(4):\n",
    "\n",
    "    se_threshold = se_thresholds[i]\n",
    "    se_flag = se_flags[i]\n",
    "\n",
    "    data = initial_data.copy()\n",
    "    G = initial_graph.copy()\n",
    "\n",
    "    run = str(k) + str(i)\n",
    "    #run = i\n",
    "\n",
    "    rep = abs(data[data['pol_inclination'] < 0]['pol_inclination'].sum())\n",
    "    dem = abs(data[data['pol_inclination'] > 0]['pol_inclination'].sum())\n",
    "    initial_pol = round((rep + dem)/data.shape[0], 6) \n",
    "    initial_homophily = nx.attribute_assortativity_coefficient(G, \"pol_inclination\")\n",
    "\n",
    "    save_data(run, 0, initial_data, 1)\n",
    "#     for i in range(2):\n",
    "\n",
    "    data = initial_data.copy()\n",
    "    G = initial_graph.copy()\n",
    "    data, G, sharing_details, net_polarization, network_homophily, polarity, user_satisfaction = start_simulation(data, G, post_conf, n_issues, lower_attd_th, upper_attd_th)\n",
    "    net_polarization = initial_pol + net_polarization\n",
    "    network_homophily = initial_homophily + network_homophily\n",
    "    save_results_to_dir(run, i, data, sharing_details, net_polarization, network_homophily, polarity, user_satisfaction)\n",
    "    save_graph(run, i, net_polarization, 1)\n",
    "    save_graph(run, i, network_homophily, 0)\n",
    "    save_graph(run, i, polarity, 2)\n",
    "    save_graph(run, i, user_satisfaction, 3)\n",
    "    save_data(run, i, data, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp 2: Balanced Coverage on Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_issues = [2,4,8,16]\n",
    "num_issues = 6\n",
    "for i in range(3):\n",
    "\n",
    "    #n_issues = num_issues[i]\n",
    "\n",
    "    if(i == 0):\n",
    "        #initial_graph, initial_data = Data.get_social_network(num_issues)\n",
    "        post_conf = Data.generate_posts(415, num_issues)\n",
    "        #print(post_conf['issue'].shape, post_conf['issue'].value_counts())\n",
    "    elif(i == 1):\n",
    "        #initial_graph, initial_data = Data.get_social_network(2)\n",
    "        #initial_graph, initial_data = Data.add_issue_stance(initial_data, initial_graph, [2,3], num_issues)\n",
    "        post_conf = Data.generate_skewed_posts(415, num_issues, 0, 0.33)\n",
    "        #print(post_conf.shape, post_conf['issue'].value_counts())\n",
    "    elif(i == 2):\n",
    "        #initial_graph, initial_data = Data.get_social_network(4)\n",
    "        #initial_graph, initial_data = Data.add_issue_stance(initial_data, initial_graph, [4,5,6,7], n_issues)\n",
    "        post_conf = Data.generate_skewed_posts(415, num_issues, 0, 0.67)\n",
    "        #print(post_conf['issue'].shape, post_conf['issue'].value_counts())\n",
    "\n",
    "    data = initial_data.copy()\n",
    "    G = initial_graph.copy()\n",
    "\n",
    "    #run = str(k) + str(i)\n",
    "    run = i\n",
    "\n",
    "    rep = abs(data[data['pol_inclination'] < 0]['pol_inclination'].sum())\n",
    "    dem = abs(data[data['pol_inclination'] > 0]['pol_inclination'].sum())\n",
    "    initial_pol = round((rep + dem)/data.shape[0], 6) \n",
    "    \n",
    "    temp_G = G.copy()\n",
    "\n",
    "    node_attr = data.set_index('id').to_dict('index')\n",
    "    nx.set_node_attributes(temp_G, node_attr)\n",
    "\n",
    "    for n in temp_G.nodes:\n",
    "        node_attr[n]['pol_inclination'] = round(node_attr[n]['pol_inclination'], 1)\n",
    "\n",
    "    #         print(temp_G.nodes)\n",
    "    #print(node_attr)\n",
    "    nx.set_node_attributes(temp_G, node_attr)\n",
    "    \n",
    "    initial_homophily = nx.attribute_assortativity_coefficient(temp_G, \"pol_inclination\")\n",
    "\n",
    "    save_data(run, 0, initial_data, 1)\n",
    "#     for i in range(2):\n",
    "\n",
    "    data = initial_data.copy()\n",
    "    G = initial_graph.copy()\n",
    "    data, G, sharing_details, net_polarization, network_homophily, polarity, user_satisfaction = start_simulation(data, G, post_conf, n_issues, lower_attd_th, upper_attd_th)\n",
    "    net_polarization = initial_pol + net_polarization\n",
    "    network_homophily = initial_homophily + network_homophily\n",
    "    save_results_to_dir(run, i, data, sharing_details, net_polarization, network_homophily, polarity, user_satisfaction)\n",
    "    save_graph(run, i, net_polarization, 1)\n",
    "    save_graph(run, i, network_homophily, 0)\n",
    "    save_graph(run, i, polarity, 2)\n",
    "    save_graph(run, i, user_satisfaction, 3)\n",
    "    save_data(run, i, data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_df = pd.read_csv('../results/initial_data.csv')\n",
    "# post_conf = pd.read_csv('../results/post_conf.csv')\n",
    "# initial_graph = Data.get_fb_network(saved_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_graph.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp 3: Varying Tolerance level in users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number_of_issues = [2,4,8,16]\n",
    "se_flag = False\n",
    "sjt_flag= True\n",
    "se_threshold = 0\n",
    "\n",
    "n_issues= 6\n",
    "# initial_graph, initial_data = Data.get_social_network(n_issues)\n",
    "# initial_data = pd.read_csv('../results/initial_data.csv')\n",
    "# initial_graph =  Data.get_fb_network(initial_data)\n",
    "# post_conf = Data.generate_posts(4000, n_issues)\n",
    "\n",
    "l_attd_th = [0.3, 0.6, 0.9]\n",
    "u_attd_th = [1.7, 1.4, 1.1]\n",
    "\n",
    "# for k in range(5):\n",
    "k = 2\n",
    "for i in range(3):\n",
    "\n",
    "    #n_issues = num_issues[i]\n",
    "    lower_attd_th = l_attd_th[i]\n",
    "    upper_attd_th = u_attd_th[i]\n",
    "\n",
    "#     if(i == 0):\n",
    "# #         initial_graph, initial_data = Data.get_social_network(num_issues)\n",
    "#         saved_df = pd.read_csv('../results/initial_data.csv')\n",
    "#         post_conf = pd.read_csv('../results/post_conf.csv')\n",
    "#         initial_graph = Data.get_saved_network(saved_df)\n",
    "#         initial_data = saved_df.copy()\n",
    "# #         sharing_details = pd.read_csv('../results/results_0/results_tollorant_Most.csv')\n",
    "# #         post_conf = sharing_details[['party_mentioned', 'post_stance', 'author_id']].rename(columns = {'party_mentioned': 'issue', 'post_stance': 'stance'})\n",
    "# #         #print(post_conf['issue'].shape, post_conf['issue'].value_counts())\n",
    "        \n",
    "#     elif(i == 1):\n",
    "#         #initial_graph, initial_data = Data.get_social_network(2)\n",
    "#         #initial_graph, initial_data = Data.add_issue_stance(initial_data, initial_graph, [2,3], n_issues)\n",
    "#         #post_conf = Data.generate_skewed_posts(415, num_issues, 0, 0.33)\n",
    "        \n",
    "#         saved_df = pd.read_csv('../results/results_1/initial_data.csv')\n",
    "#         initial_graph = Data.get_saved_network(saved_df)\n",
    "#         initial_data = saved_df.copy()\n",
    "#         sharing_details = pd.read_csv('../results/results_1/results_tollorant_Mid.csv')\n",
    "#         post_conf = sharing_details[['party_mentioned', 'post_stance', 'author_id']].rename(columns = {'party_mentioned': 'issue', 'post_stance': 'stance'})\n",
    "        \n",
    "#         #print(post_conf.shape, post_conf['issue'].value_counts())\n",
    "        \n",
    "#     elif(i == 2):\n",
    "#         #initial_graph, initial_data = Data.get_social_network(4)\n",
    "#         #initial_graph, initial_data = Data.add_issue_stance(initial_data, initial_graph, [4,5,6,7], n_issues)\n",
    "#         #post_conf = Data.generate_skewed_posts(415, num_issues, 0, 0.67)\n",
    "        \n",
    "#         saved_df = pd.read_csv('../results/results_2/initial_data.csv')\n",
    "#         initial_graph = Data.get_saved_network(saved_df)\n",
    "#         initial_data = saved_df.copy()\n",
    "#         sharing_details = pd.read_csv('../results/new_results_tolerance/results_2/results_tollorant_Least.csv')\n",
    "#         post_conf = sharing_details[['party_mentioned', 'post_stance', 'author_id']].rename(columns = {'party_mentioned': 'issue', 'post_stance': 'stance'})\n",
    "        \n",
    "        #print(post_conf['issue'].shape, post_conf['issue'].value_counts())\n",
    "\n",
    "    data = initial_data.copy()\n",
    "    G = initial_graph.copy()\n",
    "\n",
    "    run = str(k) + str(i)\n",
    "#     run = i\n",
    "\n",
    "    rep = abs(data[data['pol_inclination'] < 0]['pol_inclination'].sum())\n",
    "    dem = abs(data[data['pol_inclination'] > 0]['pol_inclination'].sum())\n",
    "    initial_pol = round((rep + dem)/data.shape[0], 6) \n",
    "    \n",
    "    temp_G = G.copy()\n",
    "        \n",
    "    node_attr = data.set_index('id').to_dict('index')\n",
    "    nx.set_node_attributes(temp_G, node_attr)\n",
    "\n",
    "    for n in temp_G.nodes:\n",
    "        node_attr[n]['pol_inclination'] = round(node_attr[n]['pol_inclination'], 1)\n",
    "\n",
    "#         print(temp_G.nodes)\n",
    "    #print(node_attr)\n",
    "    nx.set_node_attributes(temp_G, node_attr)\n",
    "    initial_homophily = nx.attribute_assortativity_coefficient(temp_G, \"pol_inclination\")\n",
    "\n",
    "    save_data(run, 0, initial_data, 1)\n",
    "#     for i in range(2):\n",
    "\n",
    "    data = initial_data.copy()\n",
    "    G = initial_graph.copy()\n",
    "    data, G, sharing_details, net_polarization, network_homophily, polarity, user_satisfaction = start_simulation(data, G, post_conf, n_issues, lower_attd_th, upper_attd_th)\n",
    "    net_polarization = initial_pol + net_polarization\n",
    "    network_homophily = initial_homophily + network_homophily\n",
    "    save_results_to_dir(run, i, data, sharing_details, net_polarization, network_homophily, polarity, user_satisfaction)\n",
    "    save_graph(run, i, net_polarization, 1)\n",
    "    save_graph(run, i, network_homophily, 0)\n",
    "    save_graph(run, i, polarity, 2)\n",
    "    save_graph(run, i, user_satisfaction, 3)\n",
    "    save_data(run, i, data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
